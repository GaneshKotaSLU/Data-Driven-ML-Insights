{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Data Science Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "from PIL import UnidentifiedImageError\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "!pip install pillow\n",
    "from PIL import Image\n",
    "!pip install matplotlib Pillow\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Constants\n",
    "BATCH_SIZE = 32\n",
    "TARGET_SIZE = (224, 224)\n",
    "df = \"/Users/ganeshkota/Desktop/Animals/raw-img/\"\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(dataset_path):\n",
    "    image_dir = Path(dataset_path)\n",
    "    filepaths = list(image_dir.glob(r'**/*.JPG')) + list(image_dir.glob(r'**/*.jpg')) + list(image_dir.glob(r'**/*.jpeg')) + list(image_dir.glob(r'**/*.PNG'))\n",
    "    labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "    filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "    image_df = pd.concat([filepaths, labels], axis=1)\n",
    "    return image_df\n",
    "\n",
    "df_preprocessed = load_and_preprocess_data(df)\n",
    "\n",
    "# Check for corrupted images within the dataset\n",
    "def check_corrupted_images(dataset_path):\n",
    "    path = Path(dataset_path).rglob(\"*.jpg\")\n",
    "    for img_p in path:\n",
    "        try:\n",
    "            img = PIL.Image.open(img_p)\n",
    "        except PIL.UnidentifiedImageError:\n",
    "            print(img_p)\n",
    "\n",
    "check_corrupted_images(df)\n",
    "\n",
    "# Display label distribution\n",
    "def display_label_distribution(image_df):\n",
    "    label_counts = image_df['Label'].value_counts()\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n",
    "    sns.barplot(x=label_counts.index, y=label_counts.values, alpha=0.8, palette='pastel', ax=axes)\n",
    "    axes.set_title('Distribution of Labels in Image Dataset', fontsize=16)\n",
    "    axes.set_xlabel('Label', fontsize=14)\n",
    "    axes.set_ylabel('Count', fontsize=14)\n",
    "    axes.set_xticklabels(label_counts.index, rotation=45)\n",
    "    fig.suptitle('Image Dataset Label Distribution', fontsize=20)\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "display_label_distribution(df_preprocessed)\n",
    "\n",
    "# Display 16 pictures of the dataset with their labels\n",
    "def display_images(image_df):\n",
    "    random_index = np.random.randint(0, len(image_df), 10)\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 10), subplot_kw={'xticks': [], 'yticks': []})\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(plt.imread(image_df.Filepath[random_index[i]]))\n",
    "        ax.set_title(image_df.Label[random_index[i]])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images(df_preprocessed)\n",
    "\n",
    "# Separate data into train and test sets\n",
    "def split_data(image_df, test_size=0.2):\n",
    "    train_df, test_df = train_test_split(image_df, test_size=test_size, shuffle=True, random_state=42)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_split, test_split = split_data(df_preprocessed)\n",
    "train_split.shape\n",
    "test_split.shape\n",
    "\n",
    "def preprocess_images(filepaths, labels):\n",
    "    processed_images = []\n",
    "    for filepath, label in zip(filepaths, labels):\n",
    "        try:\n",
    "            image = plt.imread(filepath)\n",
    "            processed_image = resize_image(image)\n",
    "            processed_images.append((processed_image, label))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error reading image {filepath}: {e}\")\n",
    "    return processed_images\n",
    "\n",
    "def resize_image(image):\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    return resized_image\n",
    "\n",
    "def preprocess_data(train_df, test_df):\n",
    "    train_split_images = preprocess_images(train_df['Filepath'].values, train_df['Label'].values)\n",
    "    test_split_images = preprocess_images(test_df['Filepath'].values, test_df['Label'].values)\n",
    "    return train_split_images, test_split_images\n",
    "\n",
    "train_split.columns\n",
    "train_split.head()\n",
    "preprocess_data(train_split, test_split)\n",
    "train_split_images, test_split_images = preprocess_data(train_split, test_split)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def augment_data(train_images):\n",
    "    augment = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        rotation_range=0.1,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "\n",
    "    # Extract file paths from the list of tuples\n",
    "    filepaths = [image[0] for image in train_images]\n",
    "    labels = [image[1] for image in train_images]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    train_df = pd.DataFrame({'Image': filepaths, 'Label': labels})\n",
    "\n",
    "    # Ensure all values in the 'Image' column are strings\n",
    "    train_df['Image'] = train_df['Image'].astype(str)\n",
    "\n",
    "    augmented_images = augment.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Image',\n",
    "        y_col='Label',\n",
    "        target_size=TARGET_SIZE,\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    return augmented_images\n",
    "\n",
    "# Separate processed images and labels from the tuples\n",
    "train_split_processed_images = [image[0] for image in train_split_images]\n",
    "train_split_labels = [image[1] for image in train_split_images]\n",
    "\n",
    "val_split_processed_images = [image[0] for image in test_split_images]\n",
    "val_split_labels = [image[1] for image in test_split_images]\n",
    "\n",
    "# Call augment_data with processed images and labels\n",
    "augment_images_result = augment_data(list(zip(train_split_processed_images, train_split_labels)))\n",
    "\n",
    "# Load pretrained model\n",
    "def load_pretrained_model():\n",
    "    pretrained_model = tf.keras.applications.efficientnet.EfficientNetB7(\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        pooling='max'\n",
    "    )\n",
    "    pretrained_model.trainable = False\n",
    "    return pretrained_model\n",
    "\n",
    "pretrained_model = load_pretrained_model()\n",
    "\n",
    "# Build and compile the model\n",
    "def build_and_compile_model(pretrained_model):\n",
    "    # Data Augmentation Step\n",
    "    augment = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.Resizing(224, 224),\n",
    "        layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "        layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "        layers.experimental.preprocessing.RandomContrast(0.1),\n",
    "    ])\n",
    "\n",
    "    inputs = pretrained_model.input\n",
    "    x = augment(inputs)\n",
    "    x = Dense(128, activation='relu')(pretrained_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.45)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.45)(x)\n",
    "    outputs = Dense(10, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(0.01),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "build_compile = build_and_compile_model(pretrained_model)\n",
    "train_data_generator = augment_data(list(zip(train_split_processed_images, train_split_labels)))\n",
    "val_data_generator = augment_data(list(zip(val_split_processed_images, val_split_labels)))\n",
    "\n",
    "# # Train the model\n",
    "# def train_model(model, train_images, val_images, epochs=1):\n",
    "#     history = model.fit(\n",
    "#         train_images,\n",
    "#         steps_per_epoch=len(train_images),\n",
    "#         validation_data=val_images,\n",
    "#         validation_steps=len(val_images),\n",
    "#         epochs=epochs,\n",
    "#     )\n",
    "#     return history\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, train_generator, val_generator, epochs=1):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Call the modified train_model function\n",
    "train_model(build_compile, train_data_generator, val_data_generator, epochs=5)\n",
    "\n",
    "# Predict the label of the test_images\n",
    "pred = build_compile.predict(val_data_generator)\n",
    "pred = np.argmax(pred, axis=1)\n",
    "\n",
    "# Map the label\n",
    "labels = (train_data_generator.class_indices)\n",
    "labels = dict((v, k) for k, v in labels.items())\n",
    "pred = [labels[k] for k in pred]\n",
    "\n",
    "# Display the result\n",
    "print(f'The first 5 predictions: {pred[:5]}')\n",
    "\n",
    "# Display 25 random pictures from the dataset with their labels\n",
    "random_index = np.random.randint(0, len(test_split) - 1, 15)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(25, 15), subplot_kw={'xticks': [], 'yticks': []})\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(plt.imread(test_split.Filepath.iloc[random_index[i]]))\n",
    "    if test_split.Label.iloc[random_index[i]] == pred[random_index[i]]:\n",
    "        color = \"Orange\"\n",
    "    else:\n",
    "        color = \"Black\"\n",
    "    ax.set_title(f\"True: {test_split.Label.iloc[random_index[i]]}\\nPredicted: {pred[random_index[i]]}\", color=color)\n",
    "plt.show()\n",
    "\n",
    "# Display confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_test = list(test_split.Label)\n",
    "print(classification_report(y_test, pred))\n",
    "confusion_matrix(y_test, pred)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
